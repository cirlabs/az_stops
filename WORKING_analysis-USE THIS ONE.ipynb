{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing AZ traffic stops\n",
    "\n",
    "This tutorial is built in Python, a powerful programming language with a wide variety of applications. In addition to Python we will need to bring in some additional tools to make our jobs easier. These tools are:\n",
    "\n",
    "- [Jupyter Notebook](http://jupyter.org/) - An environment for writing Python and other languages interactively.\n",
    "- [Pandas](https://pandas.pydata.org/) - an open-source toolkit widely used for data analysis. \n",
    "- [Altair](https://altair-viz.github.io) - a charting library used in conjunction with Pandas to display data visually.\n",
    "\n",
    "We're working in Jupyter Notebook right now. Each chunk of code or documentation is written into a cell. To run the code in each cell, press `SHIFT + return`. You should run the cells in order -- top to bottom. Otherwise, you may see error messages if you try to run code cells that rely on earlier cells that have not yet been run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in writing Python is to import tools like Pandas and Altair before we can start working with them. We also want to assign them shorter names so we don't have to type so much and because some abbreviations are commonly used -- might as well follow the convention to make your life easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we are at it, we are going to set an option so Pandas will display all the columns when we ask it to. We also have to enable Altair so we can see Altair's fancy charts later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "Now that our tools are in place we can load the data. To do so we will use a **function** built into Pandas. It's easiest to think of functions as tools that conduct a task. These tools can take different **arguments**, which are variables that help functions complete their tasks. In this case we will use Pandas' `read_csv` function. In its simplest form, the function takes one argument - the path to the file we want to open.\n",
    "\n",
    "We're basically going to tell Pandas where our data is located - in this case it is at `./data/az_2014.csv`.\n",
    "\n",
    "We're also going to pass on additional argument to the `read_csv` function: a `dtype`. Since our spreadsheet data mostly consists of categories (e.g. the race of a driver) rather than numbers (e.g. the driver was going 53 mph) we are going to tell Pandas to import everything in the spreadsheet as if it was just text using the `dtype=str` argument. This has the added benefit of suppressing a warning issued by Pandas because it cannot determine the data types for each column.\n",
    "\n",
    "Now, let's use Pandas to load our csv data into a new variable called `az_stops`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting to know our data\n",
    "\n",
    "One of the first steps in data analysis is getting a general feel for the data. What does it look like? How many columns are there? Is any data missing? Let's start answering some of these questions.\n",
    "\n",
    "We'll use a function called `head` to look at the top of the dataset. It's very similar to the command-line `head` tool we used earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a general idea as to what columns we have and what the data looks like. Notice all the `NaN` (Not a Number) values. We will talk about those shortly. Keep in mind that this is only the first few rows. Now let's use a different function to take a more comprehensive look at the columns and how they are made up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list showing information about each column, including:\n",
    "\n",
    "- The column name\n",
    "- The number of records which have data in each column\n",
    "- Whether or not there is missing or null data (see below)\n",
    "- The data type of the column - in this case everything is listed as an `object`, which is how Pandas describes text fields.\n",
    "\n",
    "Additionally we get information on how many records are in our data - this translates directly to the number of traffic stops we are analyzing. You will notice this matches up with the counts we checked earlier when extracting the data.\n",
    "\n",
    "**Note:** One side effect of our importing our fields as `str` is that Pandas will handle missing, null or blank data (those `NaN` values from above) a little differently than it does by default. Normally Pandas would automatically exclude null values from any sort of analysis. We don't necessarily want that. It may be important to keep track of how many records do not have actual data in it for a given column. By telling Pandas that everything should be rendered as text when we import the data it allows us to keep null, missing or blank values in the analysis. But don't worry: we will talk about how to handle those later.\n",
    "\n",
    "### Diving a little deeper\n",
    "\n",
    "Now that we have a general sense of of our data, let's do some preliminary analysis to see what else we can learn.\n",
    "\n",
    "One of the first things we want to learn is if there are any gaps in our data that we haven't been able to see so far. Let's start by taking a look at traffic stops by month. To do this we will work with the `stop_date` column, but we will need to convert it to the proper data type -- a Python `datetime` instead of an object -- before we can perform an analysis on it. Luckily, Pandas has a handy function to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that syntax is pretty weird -- what just happened? Essentially, we told Pandas to rewrite the `stop_date` field for each row in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our `stop_date` column is in the correct format, we want to get a count of stops by month to check if we have the entire year. To do this we will use the `groupby` function. \n",
    "\n",
    "Since we only want to count stops by month we should try to count a column that has no missing data - in this case it would be the `id` column. We isolate that column by using the `az_stops['id']` syntax.\n",
    "\n",
    "The `groupby` function takes several different arguments, but only one is required - which field you want to group by. In this case we need to group by the month of the `stop_date` column. Because we turned our stop_date into a fancy Python date, we can access that using this method: `az_stops.stop_date.dt.month`.\n",
    "\n",
    "Lastly, since `groupby` is meant to aggregate data using different mathmatical functions (think sum, average, count, etc) we need to specify what kind of math we want to use when aggregating. In this case, we want to count how many stops are in each month, so we use `count()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking a visual look at our data\n",
    "\n",
    "It's nice to have the table showing our counts by month, but sometimes we want more detail. Let's look at traffic stops by day of the yar. For that much data, we probably want to look at this visually rather than in a table, so let's build a bar chart.\n",
    "\n",
    "First we need to do another `groupby`, but this time the syntax will be a bit different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we draw the chart, let's change the name of the `id` column into something that makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can draw the chart using Altair. Altair requires us to use several different functions tied together to display the chart. The first function is just starting up a chart and telling it what data we want to use. Then we tell it what type of chart we want (a line or fever chart in this case). Lastly we tell it what columns in our data should be used for the chart axes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks fine. We certainly can get a better idea of our data. But can we make it look better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's something fishy here. You can see there are spikes throughout the year where there are more stops than normal -- sometimes the stops nearly double from one day to the next. Let's see if we can find out more information about those spikes. We can do this by filtering the `stops_by_day` data to show only days where more than 2,000 stops occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We really don't have a good answer to what is going on here. For some reason, traffic stops seem to spike towards the middle of the month. Sometimes it happens at the beginning of the month. Maybe some spikes could be related to holiday enforcement actions such as Fourth of July and Thanksgiving DUI checkpoints? But at this point, we don't know. This is a great example of a case where you would need to do additional reporting to try to figure out what's going on before publishing results of this data. However, this shouldn't impact our training analysis greatly because time is not a factor in what we want to uncover."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work on your own, part I\n",
    "\n",
    "Now that we have taken a fairly in-depth look at the `stop_date` column, let's take a closer look at counties where the stops occurred. Use the `county_name` column to generate your own table of traffic stops throughout the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coming back\n",
    "\n",
    "So now we have an idea of how many traffic stops occurred in each county. For the purposes of our analysis this actually means very little since we will be taking a look at the entire state. However if you wanted to localize this to your area it is probably a good idea to take a look at how many stops are occurring in your coverage area.\n",
    "\n",
    "## Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we make this look better? First let's drop the \" County\" at the end of every county name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digging into the demographics\n",
    "\n",
    "Now we can start playing with demographic data that we have in the data set. Our initial look at the data showed us that there isn't any information on the age of the driver, but we do have pretty good data on gender and race/ethnicity. Rather than examining both gender and race separately, let's see if we can do an analysis on both columns at the same time.\n",
    "\n",
    "To do this we will use what is called a pivot table. Those of you who have worked with Excel may be familiar with pivot tables. They are a way to restructure your data to provide a different perspective.\n",
    "\n",
    "Before we get started though, let's do some housekeeping. The `driver_gender` column identifies gender by abbreviation. Let's change that to a full spelling. Earlier, we used `rename` to change the column name, but now we're changing the data, not the column name. So we'll use the `replace` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better. While we are at it let's take a peek at the `driver_race` column to see how race is coded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what we want to know is the number of stops by gender and race. Let's build our pivot table to show us that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our results. But again, data presented this way is a bit difficult to for humans to read easily. So we should probably chart this out as well. First we have to clean the table up a bit. We need to get rid of the totals and restructure the dataframe a bit.\n",
    "\n",
    "But wait a minute. What does `melt` mean? Basically it means we're converting \"wide\" data to \"long\" data to show combinations of variables together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do we have a story yet?\n",
    "\n",
    "At this point it doesn't look like it. As you can see in the graphic above, whites account for a large majority of the stops. And men got stopped more than women, regardless of race. But what is wrong with this analysis?\n",
    "\n",
    "In short it doesn't tell us whether or not minority groups are over- or under-represented in the data. To find that out we would need to compare stops by race to some other metric. One option would be Census data showing the racial make up of the state's population. Another choice would be to look at demographic breakdowns of licensed drivers. This sort of information is usually requested from the state's Department of Motor Vehicles.\n",
    "\n",
    "However there is a flaw in this sort of analysis. These are stops conducted by the Arizona Department of Public Safety, which means most stops were conducted on highways or state routes. This means drivers stopped by the DPS are more likely to be from outside the state – or given the proximity to the border – from Mexico. So population figures from a secondary source would not necessarily represent the population on the road.\n",
    "\n",
    "Fortunately there is another metric in the data that we can use to look for bias – search patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing search patterns\n",
    "\n",
    "Another metric that could show bias is whether or not a search was conducted during the course of the traffic stop. For our purposes there are three different types of searches – probable cause, warrant and consent searches. The ones we want to focus on are consent searches. This type of search does not require a search warrant, and the officer most likely does not have probable cause. As such this search is completely up to the officer's discretion.\n",
    "\n",
    "There is a `consent_search` column in the data set, but let's explore a bit more before we dive into our analysis.\n",
    "\n",
    "Let's start by getting a better feel for how many searches were conducted overall. Rather than do a `groupby` let's build a pivot table so we can get some totals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's take a look at the number of consent searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a problem here. The total number from our `search_conducted` pivot table does not match the total from our `consent_search` pivot table.\n",
    "\n",
    "Why is this happening? In some cases, we can't determine if `consent_search` is true or false because sometimes the variable is null.\n",
    "\n",
    "We can fix this by cleaning the data. We can start by getting a count of the null values in the `consent_search` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those null values are cases where a probable cause or warrant search was conducted. Let's replace those null values with a value of `unknown_search`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to filter for cases where a search was conducted but it was not a consent search. We will replace those values with `unknown_search` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this called a `mask`? This is a pandas convention -- it means we're creating a filter that can be applied over our dataframe to mask out other data.\n",
    "\n",
    "And `loc` is taking the rows that matches our filter, and then putting the value of \"unknown_search\" into the `consent_search` field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we will rename `TRUE` values as `consent_search` and any remaining `FALSE` values as `no_search`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build a pivot table of the consent searches and searches conducted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work on your own, part II\n",
    "\n",
    "Now that we have the `consent_search` column cleaned up, we can start doing our analysis. The first step is to create a pivot table with `consent_search` as the columns, `driver_race` as the race and a count of `id` as the values. Call it `search_rates`. Make sure to convert it back to a dataframe after you are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coming back\n",
    "\n",
    "Now we are starting to get somewhere. We now know the number of consent searches, the number of other searches and the total number of stops by race. This is enough for us to start doing some math.\n",
    "\n",
    "## Calculating search rates\n",
    "\n",
    "Here we will need to add a new column to our data - a ratio of searches to stops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we have a story...\n",
    "\n",
    "A little math was able to do a lot in this case. We can now see that comparatively speaking, Hispanics were searched at rates much higher than whites in 2015. In fact they were searched at a rate nearly twice that of any other racial/ethnic group. So how do we write about that? What are some sentences that we could write in a story? What is the best way to present those numbers visually?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work on your own, part III\n",
    "\n",
    "Now that we have some results, let's create a simple bar chart to display them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But what if...\n",
    "\n",
    "Some critics may argue that officers have good reason when they search somebody's vehicle and that reason may not be race. Fortunately the data contains a column that can help us check that – `contraband_found`. Basically we can check to see how often a consent_search actually turned up something illegal. \n",
    "\n",
    "### Work on your own, part III\n",
    "\n",
    "This exercise has multiple parts:\n",
    "1. First you need to filter the `az_stops` dataframe so you are looking at only consent searches. \n",
    "2. Then you need to create a pivot table called `hit_rates` based on the `driver_race` and the `contraband_found` columns. Convert it back to a dataframe.\n",
    "3. Create a new `hit_rate` column and calculate the hit rate per 100 stops.\n",
    "4. Rename the driver_race 'total' row to 'All races'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coming back\n",
    "\n",
    "Whoa. It looks like we found something amazing here. **Not only are Hispanics searched at rates much higher than any other racial/ethnicity but police find contraband on them at rates lower than any other racial/ethnic group.**\n",
    "\n",
    "But rather than scrolling up and down the page here, let's combine our results into a single table. To do this we will use the merge function in Pandas. (If you have worked with SQL, this is very similar to a SQL join.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's clean up the columns a bit. We want to rename the `total_x` to `traffic_stops` and drop the `total_y` column since it duplicates `consent_search`. We can drop the other search columns as well as the `miss` column. We also should rename the `search_rate` and `hit_rate` columns to make more sense. While we are at it, let's re-order the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's visualize it\n",
    "\n",
    "Again it may help to visualize the data. Let's create a scatter plot comparing hit rates and search rates by race."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we make this better?\n",
    "\n",
    "1. Multiple years of analysis\n",
    "2. Break up by county?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
